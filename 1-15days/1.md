
# LeNet

## 简介
LeNet-5(http://yann.lecun.com/exdb/lenet/a35.tml), 一个手写体数字识别模型，是一个广为人知的商用的卷积神经网络。当年美国大多数银行用它来识别支票上面的手写数字。

![lenet](http://yann.lecun.com/exdb/lenet/gifs/a35.gif)


## 基本信息
论文：基于梯度的学习在文档识别中的应用
paper ：LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). "Gradient-based learning applied to document recognition"
http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf
作者：Yann LeCun，LéonBottou，Yoshua Bengio和Patrick Haffner
发表于：IEEE会议论文集（1998）

## 创新点

它是深度学习最简单的体系结构之一。 它具有2个卷积层和3个完全连接层。因此LeNet被称为LeNet-5。平均池化层现在称为降采样层，并且当时它具有可训练的权重（现在很少使用这种设计，往往使用Max-pooling）。 该体系结构具有约60,000个参数。


它的网络结构成为后来深度学习网络的标准的“模板”：堆叠若干激活功能的卷积层和pooling层，最后加上一个或多个完全连接的FC层。


## 网络结构


Lenet5 原始结构如，包括卷积层，降采样，卷积层，降采样，卷积层（实现全连接），全连接层，高斯连接层（进行分类）

![lenet org model](https://github.com/weslynn/graphic-deep-neural-network/blob/master/pic/basicpic/lenet-org.jpg)


LeNet-5网络参数配置

|     网络层      |       输入尺寸       |          核尺寸          |       输出尺寸       |          可训练参数量           |
| :-------------: | :------------------: | :----------------------: | :------------------: | :-----------------------------: |
|   卷积层$C_1$   | $32\times32\times1$  |  $5\times5\times1/1,6$   | $28\times28\times6$  |  $(5\times5\times1+1)\times6$   |
|  下采样层$S$  | $28\times28\times6$  |       $2\times2/2$       | $14\times14\times6$  |       $(1+1)\times6$ $^*$       |
|   卷积层$C_2$   | $14\times14\times6$  |  $5\times5\times6/1,16$  | $10\times10\times16$ |            $1516^*$             |
|  下采样层$S$  | $10\times10\times16$ |       $2\times2/2$       |  $5\times5\times16$  |         $(1+1)\times16$         |
|  全连接层$F_3$$^*$ |  $5\times5\times16$  | $5\times5\times16/1,120$ | $1\times1\times120$  | $(5\times5\times16+1)\times120$ |
|  全连接层$F_4$  | $1\times1\times120$  |      $120\times84$       |  $1\times1\times84$  |        $(120+1)\times84$        |
| 全连接层$F_5$/输出层|  $1\times1\times84$  |       $84\times10$       |  $1\times1\times10$  |        $(84+1)\times10$         |


可显示如图

$L(p, t, x) = L_{cls}(p, p^*) + {\lambda}L_{reg}(t, t^*) + {\mu}L_c(x)$
其中降采样层在后期发展中被maxpooling所取代，分类也被softmax所替代，第一个全连接层，可以用卷积层来替代。因此很多实现中会改成3个卷积层，2全连接层来进行实现，如下：

一层卷积层： 5×5的卷积核，6个

一层maxpooling

一层卷积层：5×5的卷积核，16个

一层maxpooling

一层卷积层：5×5的卷积核，120个

一层全连接层：84个隐含节点，激活函数为ReLU（paper中激活函数为sigmoid）

最后通过softmax分类输出（paper之前为一个高斯连接层，由Euclidean Radial Basis Function单元组成）



数据变化为

![lenet data](https://github.com/weslynn/graphic-deep-neural-network/blob/master/modelpic/lenet_data2.png)

## 源码
比较热门的源码 有这些，现在一般pooling直接用maxpooling：

tensorflow 源码 https://github.com/tensorflow/models/tree/master/research/slim/nets/lenet.py

tensorflow的输入 改成了28×28，因此少了一层卷积层，最后使用softmax输出

pytorch 源码 https://github.com/pytorch/examples/blob/master/mnist/main.py

caffe https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt


给出一个pytorch简洁的代码示例：

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        self.fc1 = nn.Linear(16*5*5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = func.relu(self.conv1(x))
        x = func.max_pool2d(x, 2)
        x = func.relu(self.conv2(x))
        x = func.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = func.relu(self.fc1(x))
        x = func.relu(self.fc2(x))
        x = self.fc3(x)
        return x

## 知识点  

反向传播算法  /  卷积  / 激活函数  / 神经元结构  


**关于激活函数，选出下列说法中不对的答案

A 激活函数可以引入非线性因素，完成复杂的函数映射。
B 激活函数都是线性的，深度网络也可以完成学习和模拟其他复杂类型的数据，例如图像、音频等。
C 激活函数可以把当前特征空间映射到另一个空间，让数据能够更好的被分类。
D 激活函数不能都是线性的



答案解析 及更多题目 参见微信 。（把答案都列出来了，你们就不会思考了）