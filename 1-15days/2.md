
# AlexNet

## 简介
2012年，Alex Krizhevsky用AlexNet 在当年的ImageNet图像分类竞赛中(ILSVRC 2012)，以top-5错误率15.3%拿下第一。 他的top-5错误率比上一年的冠军下降了十个百分点，而且远远超过当年的第二名。它使用ReLU代替了传统的激活函数，而且网络针对多GPU训练进行了优化设计，用于提升速度。不过随着硬件发展，现在我们训练AlexNet都可以直接用简化后的代码来实现了。



## 基本信息
论文： "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012. [pdf](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

作者：Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.
发表于：2012

中英翻译  http://noahsnail.com/2017/07/04/2017-07-04-AlexNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/


## 创新点

1 ReLU函数作为激活函数
2 dropout:选择性地忽略训练中的单个神经元，避免模型的过拟合
3 max-pooling:避免平均池化（average pooling）的平均效应
4 利用双GPU NVIDIA GTX 580训练

其他： LRN层的优化，tf官方后来给出的代码，进行了修改，将初始化选择用xavier_initializer的方法，将LRN层移除了。




## 网络结构


AlexNet 原始结构如图

![alexnet org model](https://github.com/weslynn/graphic-deep-neural-network/blob/master/pic/basicpic/alexnet-org.jpg)

除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（$C_3$卷积层和$F_{6-8}$全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为$softmax$的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如下

​                                   表4.2 AlexNet网络参数配置

|        网络层         |               输入尺寸               |                  核尺寸                  |               输出尺寸               |              可训练参数量               |
| :-------------------: | :----------------------------------: | :--------------------------------------: | :----------------------------------: | :-------------------------------------: |
|   卷积层$C_1$ $^*$    |        $224\times224\times3$         | $11\times11\times3/4,48$  | $55\times55\times48$  | $(11\times11\times3+1)\times48\times2$  |
| 下采样层$S$$^*$ | $55\times55\times48$  |       $3\times3/2$        | $27\times27\times48$  |                    0                    |
|      卷积层$C_2$      | $27\times27\times48$  | $5\times5\times48/1,128$  | $27\times27\times128$ | $(5\times5\times48+1)\times128\times2$  |
|   下采样层$S$   | $27\times27\times128$ |       $3\times3/2$        | $13\times13\times128$ |                    0                    |
|   卷积层$C_3$ $^*$    |  $13\times13\times128$  | $3\times3\times256/1,192$ | $13\times13\times192$ | $(3\times3\times256+1)\times192\times2$ |
|      卷积层$C_4$      | $13\times13\times192$ | $3\times3\times192/1,192$ | $13\times13\times192$ | $(3\times3\times192+1)\times192\times2$ |
|      卷积层$C_5$      | $13\times13\times192$ | $3\times3\times192/1,128$ | $13\times13\times128$ | $(3\times3\times192+1)\times128\times2$ |
|   下采样层$S$   | $13\times13\times128$ |       $3\times3/2$        |  $6\times6\times128$  |                    0                    |
|  全连接层$F_6$  $^*$  |   $6\times6\times128$   |     $9216\times2048$      | $1\times1\times2048$  |       $(9216+1)\times2048\times2$       |
|     全连接层$F_7$     |  $1\times1\times2048$   |     $4096\times2048$      | $1\times1\times2048$  |       $(4096+1)\times2048\times2$       |
|     全连接层$F_8$     |  $1\times1\times2048$   |             $4096\times1000$             |         $1\times1\times1000$         |       $(4096+1)\times1000\times2$       |


可显示如图


模型结构如下

一层卷积层： 11×11的卷积核，96个，步长位4 （stride = 4）

一层maxpooling

一层LRN

一层卷积层：5×5的卷积核，256个，pad=2

一层maxpooling

一层LRN

一层卷积层：3×3的卷积核，384个，pad=1

一层卷积层：3×3的卷积核，384个，pad=1

一层卷积层：3×3的卷积核，256个

一层maxpooling

一层全连接层：4096个隐含节点，激活函数为ReLU

一层全连接层：4096个隐含节点，激活函数为ReLU

最后通过softmax分类输出1000类



## 源码
有这些源码：
tensorflow 源码 https://github.com/tensorflow/models/tree/master/research/slim/nets/alexnet.py

caffe https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt



给出一个pytorch简洁的代码示例：

class AlexNet(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(AlexNet, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(64, 192, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2),
        )
        self.classifier = nn.Sequential(
            nn.Dropout(),
            nn.Linear(256 * 2 * 2, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x


## 知识点
RELU /  pooling  /dropout

** 对于下列说法中，不对的是？

​A Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。
B Max Pooling 实际上就是在 n\*n 的样本中取最大值，作为采样后的样本值。
​C 当在卷积神经网络中加入池化层(pooling layer)后，变换的不变性会被改变
​D 对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。


